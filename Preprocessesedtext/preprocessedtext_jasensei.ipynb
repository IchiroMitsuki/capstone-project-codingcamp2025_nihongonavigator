{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67d2901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.data.path.append(r\"C:\\Users\\User\\AppData\\Roaming\\nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Library untuk tahap Modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Library untuk menggunakan algoritma Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0718471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RevJAsensei = pd.read_csv(r\"C:\\Latihan_Python\\casptone-project-dicoding_nihongonavigator\\ulasan_jasensei.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04947a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "jumlah_ulasan, jumlah_kolom = df_RevJAsensei.shape\n",
    "\n",
    "print(jumlah_ulasan)\n",
    "print(jumlah_kolom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa29e70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aplikasinya sangat bagus. Dari semua aplikasi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sejauh ini aplikasi ternyaman. Tidak ada iklan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aplikasi nya udah bagus tapi pas saya ingin me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paling rekomen banget di antara apk yang lain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banyak pelajaran lengkap, gratis, dan offline,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  Aplikasinya sangat bagus. Dari semua aplikasi ...\n",
       "1  sejauh ini aplikasi ternyaman. Tidak ada iklan...\n",
       "2  aplikasi nya udah bagus tapi pas saya ingin me...\n",
       "3  paling rekomen banget di antara apk yang lain ...\n",
       "4  Banyak pelajaran lengkap, gratis, dan offline,..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RevJAsensei.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92efc49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 401 entries, 0 to 400\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  401 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_RevJAsensei.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d39b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Missing Value\n",
      "Review    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah Missing Value\")\n",
    "print(df_RevJAsensei.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e329d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Duplikat Value\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah Duplikat Value\")\n",
    "print(df_RevJAsensei.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef75877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df_RevJAsensei_clean = df_RevJAsensei.drop_duplicates()\n",
    "print(df_RevJAsensei_clean.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e6fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aplikasinya sangat bagus. Dari semua aplikasi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sejauh ini aplikasi ternyaman. Tidak ada iklan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aplikasi nya udah bagus tapi pas saya ingin me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paling rekomen banget di antara apk yang lain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banyak pelajaran lengkap, gratis, dan offline,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  Aplikasinya sangat bagus. Dari semua aplikasi ...\n",
       "1  sejauh ini aplikasi ternyaman. Tidak ada iklan...\n",
       "2  aplikasi nya udah bagus tapi pas saya ingin me...\n",
       "3  paling rekomen banget di antara apk yang lain ...\n",
       "4  Banyak pelajaran lengkap, gratis, dan offline,..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RevJAsensei_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e56a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 372 entries, 0 to 400\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  372 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_RevJAsensei_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97479d7",
   "metadata": {},
   "source": [
    "## **Text PreProcessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781e1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "\n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\", \"wkwk\", \"uuu\"])\n",
    "    important_words = {'baik', 'buruk', 'mantap', 'bagus', 'jelek', 'mudah', 'cepat', 'nyaman'} #Bisa ditambah\n",
    "    # Hapus kata penting dari daftar stopwords\n",
    "    listStopwords = listStopwords - important_words\n",
    "\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    "\n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15556414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7an -> tujuan\n",
      "@ -> di\n",
      "ababil -> anak ingusan\n",
      "abis -> habis\n",
      "acc -> accord\n",
      "ad -> ada\n",
      "adlah -> adalah\n",
      "adlh -> adalah\n",
      "adoh -> aduh\n",
      "afaik -> as far as i know\n"
     ]
    }
   ],
   "source": [
    "def load_slang_dictionary():\n",
    "    slangwords = {}\n",
    "\n",
    "    def read_file(path, delimiter):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if delimiter not in line or not line:\n",
    "                    continue  # lewati baris yang kosong atau tidak valid\n",
    "                parts = line.split(delimiter, 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue  # lewati jika tetap gagal dibagi jadi 2\n",
    "                key, value = parts\n",
    "                if key and value:\n",
    "                    slangwords[key.lower()] = value.lower()\n",
    "\n",
    "    # Gabungkan dari semua file\n",
    "    read_file(r'C:\\Latihan_Python\\@-proyek-course-dicoding\\proyek_analisis_sentimen\\Indonesian Slang dictionary (from github)\\formalizationDict.txt', '\\t')\n",
    "    read_file(r'C:\\Latihan_Python\\@-proyek-course-dicoding\\proyek_analisis_sentimen\\Indonesian Slang dictionary (from github)\\kbba.txt', '\\t')\n",
    "    read_file(r'C:\\Latihan_Python\\@-proyek-course-dicoding\\proyek_analisis_sentimen\\Indonesian Slang dictionary (from github)\\slangword.txt', ':')\n",
    "\n",
    "    return slangwords\n",
    "\n",
    "# Panggil fungsi\n",
    "slangwords = load_slang_dictionary()\n",
    "\n",
    "# Contoh tampilkan 10 kata\n",
    "for i, (k, v) in enumerate(slangwords.items()):\n",
    "    if i >= 10: break\n",
    "    print(f\"{k} -> {v}\")\n",
    "\n",
    "def fix_slangwords(text, slang_dict):\n",
    "    words = text.split()  # Memecah teks jadi list kata\n",
    "    fixed_words = [slang_dict.get(word.lower(), word) for word in words]  # Ganti jika ada di kamus\n",
    "    return ' '.join(fixed_words)  # Gabung kembali jadi kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036b6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heuristic(text):\n",
    "    # Ganti koma dan kata penghubung umum dengan titik\n",
    "    text = re.sub(r'\\s*(,|namun|tapi|tetapi|meskipun|walaupun|serta)\\s*', '.', text)\n",
    "    \n",
    "    # Pisah berdasarkan titik atau baris baru\n",
    "    sentences = re.split(r'\\.|\\n', text)\n",
    "\n",
    "    # Bersihkan hasil\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 3]\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ac52441",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_normalization = {\n",
    "    \"mantaaap\": \"mantap\",\n",
    "    \"bangett\": \"banget\",\n",
    "    \"kerenn\": \"keren\",\n",
    "    \"prem\": \"premium\",\n",
    "    \"bagusss\": \"bagus\",\n",
    "    \"membantuuu\": \"membantu\",\n",
    "    \"bangettt\": \"banget\",\n",
    "    \"bingunggg\": \"bingung\",\n",
    "    \"sangattt\": \"sangat\",\n",
    "    \"waaaah\": \"wah\",\n",
    "    \"polll\": \"pol\",           \n",
    "    \"tulisannnya\": \"tulisannya\",\n",
    "    \"maziiini\": \"mazii ini\",\n",
    "    \"salahhadehhhh\": \"salah hadeh\",\n",
    "    \"sangaatttt\": \"sangat\",\n",
    "    \"lagiii\": \"lagi\",\n",
    "    \"bagusssss\": \"bagus\",\n",
    "    \"bagusssbisa\": \"bagus bisa\",\n",
    "    \"mantaaapberjamjam\": \"mantap berjam-jam\",\n",
    "    \"waaaaah\": \"wah\",\n",
    "    \"yaaa\": \"ya\",\n",
    "    \"mmmmungkin\": \"mungkin\",\n",
    "    \"bagussss\": \"bagus\",\n",
    "    \"mantappppuuu\": \"mantap\",\n",
    "    \"majuuu\": \"maju\",\n",
    "    \"bagussssss\": \"bagus\",\n",
    "    \"akuuu\": \"aku\",\n",
    "    \"hehheee\": \"hehe\",\n",
    "    \"kerennnn\": \"keren\",\n",
    "    \"makasiii\": \"makasih\",\n",
    "    \"gimanasi\": \"bagaimana sih\",\n",
    "    \"arigato\": \"terima kasih\",\n",
    "    \"arigatou\": \"terima kasih\",\n",
    "    \"radical\": \"radikal\",\n",
    "    \"knji\": \"kanji\",\n",
    "    \"knj\": \"kanji\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f13bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(tokens, norm_dict):\n",
    "    return [norm_dict.get(word, word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14b2458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_clean'] = df_RevJAsensei_clean['Review'].apply(cleaningText)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_casefoldingText'] = df_RevJAsensei_clean['text_clean'].apply(casefoldingText)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_final_splitted'] = df_RevJAsensei_clean['text_casefoldingText'].apply(split_heuristic).apply(toSentence)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_slangwords'] = df_RevJAsensei_clean['text_final_splitted'].apply(lambda x: fix_slangwords(x, slangwords))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_tokenizingText'] = df_RevJAsensei_clean['text_slangwords'].apply(tokenizingText)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_normalized'] = df_RevJAsensei_clean['text_tokenizingText'].apply(lambda x: normalize_tokens(x, custom_normalization))#Baru\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_stopword'] = df_RevJAsensei_clean['text_normalized'].apply(filteringText)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_sentenced'] = df_RevJAsensei_clean['text_stopword'].apply(toSentence)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21392\\4146587439.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_RevJAsensei_clean['text_final_stemmed'] = df_RevJAsensei_clean['text_sentenced'].apply(stemmingText)\n"
     ]
    }
   ],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "df_RevJAsensei_clean['text_clean'] = df_RevJAsensei_clean['Review'].apply(cleaningText)\n",
    "\n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "df_RevJAsensei_clean['text_casefoldingText'] = df_RevJAsensei_clean['text_clean'].apply(casefoldingText)\n",
    "\n",
    "df_RevJAsensei_clean['text_final_splitted'] = df_RevJAsensei_clean['text_casefoldingText'].apply(split_heuristic).apply(toSentence)\n",
    "\n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "df_RevJAsensei_clean['text_slangwords'] = df_RevJAsensei_clean['text_final_splitted'].apply(lambda x: fix_slangwords(x, slangwords))\n",
    "\n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "df_RevJAsensei_clean['text_tokenizingText'] = df_RevJAsensei_clean['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "df_RevJAsensei_clean['text_normalized'] = df_RevJAsensei_clean['text_tokenizingText'].apply(lambda x: normalize_tokens(x, custom_normalization))#Baru\n",
    "\n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "df_RevJAsensei_clean['text_stopword'] = df_RevJAsensei_clean['text_normalized'].apply(filteringText)\n",
    "\n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "df_RevJAsensei_clean['text_sentenced'] = df_RevJAsensei_clean['text_stopword'].apply(toSentence)\n",
    "\n",
    "df_RevJAsensei_clean['text_final_stemmed'] = df_RevJAsensei_clean['text_sentenced'].apply(stemmingText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b965a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RevJAsensei_clean.to_csv(r\"C:\\Latihan_Python\\casptone-project-dicoding_nihongonavigator\\Preprocessesedtext\\review_jasensei_processedtext.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
